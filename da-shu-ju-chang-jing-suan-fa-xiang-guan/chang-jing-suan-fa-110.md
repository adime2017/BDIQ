# 大数据场景和算法类\[1-10\]

### 试题一：有 10 个文件，每个文件 1G，每个文件的每一行存放的都是用户的 query，每个文件的query 都可能重复。要求你按照 query 的频度排序

解题方案一：

顺序读取 10 个文件，按照 hash\(query\)%10 的结果将 query 写入到另外 10 个文件（记为）中。这样新生成的文件每个的大小大约也 1G（假设 hash 函数是随机的）。 找一台内存在 2G 左右的机器，依次对用 hash\_map\(query, query\_count\)来统计每个query 出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的 query 和对应的 query\_cout 输出到文件中。这样得到了 10 个排好序的文件（记为）。 对这 10 个文件进行归并排序（内排序与外排序相结合）

解题方案二：

一般 query 的总量是有限的，只是重复的次数比较多而已，可能对于所有的 query，一次性就可以加入到内存了。这样，我们就可以采用 trie 树/hash\_map等直接来统计每个 query出现的次数，然后按出现次数做快速/堆/归并排序就可以了

解题方案三

与方案 1 类似，但在做完 hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如 MapReduce），最后再进行合并

### 试题二：在 2.5 亿个整数中找出不重复的整数，注，内存不足以容纳这 2.5 亿个整数

解题方案 1：采用 2-Bitmap（每个数分配 2bit，00 表示不存在，01 表示出现一次，10 表示多次，11 无意义）进行，共需内存 2^32 \* 2 bit=1 GB 内存，还可以接受。然后扫描这 2.5亿个整数，查看 Bitmap 中相对应位，如果是 00 变 01，01 变 10，10 保持不变。所描完事后，查看 bitmap，把对应位是 01 的整数输出即可

解题方案 2：也可采用与第 1 题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素

### 试题三：给 40 亿个不重复的 unsigned int 的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那 40 亿个数当中

解题方案一：申请 512M 的内存，一个 bit 位代表一个 unsigned int 值。读入 40 亿个数，设置相应的 bit 位，读入要查询的数，查看相应 bit 位是否为 1，为 1 表示存在，为 0 表示不存在。

解题方案二：这个问题在《编程珠玑》里有很好的描述，大家可以参考下面的思路，探讨一下： 又因为 2^32 为 40 亿多，所以给定一个数可能在，也可能不在其中； 这里我们把 40 亿个数中的每一个用 32 位的二进制来表示 ，假设这 40 亿个数开始放在一个文件中。 然后将这 40 亿个数分成两类: 1.最高位为 0 2.最高位为 1 并将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=20 亿，而另一个&gt;=20 亿（这相当于折半了）； 与要查找的数的最高位比较并接着进入相应的文件再查找 再然后把这个文件为又分成两类: 1.次最高位为 0 2.次最高位为 1 并将这两类分别写入到两个文件中，其中一个文件中数的个数&lt;=10 亿，而另一个&gt;=10 亿（这相当于折半了）； 与要查找的数的次最高位比较并接着进入相应的文件再查找。 ..... 以此类推，就可以找到了,而且时间复杂度为 O\(logn\)

> 位图方法： 使用位图法判断整形数组是否存在重复 ,判断集合中存在重复是常见编程任务之一，当集合中数据量比较大时我们通常希望少进行几次扫描，这时双重循环法就不可取了。 位图法比较适合于这种情况，它的做法是按照集合中最大元素 max 创建一个长度为 max+1的新数组，然后再次扫描原数组，遇到几就给新数组的第几位置上 1，如遇到 5 就给新数组的第六个元素置 1，这样下次再遇到 5 想置位时发现新数组的第六个元素已经是 1 了，这说明这次的数据肯定和以前的数据存在着重复。这 种给新数组初始化时置零其后置一的做法类似于位图的处理方法故称位图法。它的运算次数最坏的情况为 2N。如果已知数组的最大值即能事先给新数组定长的话效 率还能提高一倍

### 试题四：怎么在海量数据中找出重复次数最多的一个？

解题方案一：先做 hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求

### 试题五：上千万或上亿数据（有重复），统计其中出现次数最多的钱 N 个数据

解题方案一：上千万或上亿的数据，现在的机器的内存应该能存下。所以考虑采用 hash\_map/搜索二叉树/红黑树等来进行统计次数。然后就是取出前 N 个出现次数最多的数据了，可以用第 2 题提到的堆机制完成。

### 试题六：一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前 10 个词，给出思想，给出时间复杂度分析？

解题方案一：这题是考虑时间效率。用 trie 树统计每个词出现的次数，时间复杂度是 O\(n_le\)（le表示单词的平准长度）。然后是找出出现最频繁的前 10 个词，可以用堆来实现，前面的题中已经讲到了，时间复杂度是 O\(n_lg10\)。所以总的时间复杂度，是 O\(n_le\)与 O\(n_lg10\)中较大的哪一个



### 试题七：从100w 个数中找出最大的 100 个数。   

解题方案一： 用一个含 100 个元素的最小堆完成。复杂度为O\(100w\*lg100\)。   

解题方案二：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比 100 多的时候，采用传统排序算法排序，取前 100 个。复杂度为 O\(100w\*100\)。   

解题方案三：采用局部淘汰法。选取前 100 个元素，并排序，记为序列 L。然后一次扫描剩余的元素 x，与排好序的 100 个元素中最小的元素比，如果比这个最小的 要大，那么把这个最小的元素删除，并把 x 利用插入排序的思想，插入到序列 L 中。依次循环，直到扫描了所有的元素。复杂度为 O\(100w\*100\)。 

###  试题八：有一千万条短信，有重复，以文本文件的形式保存，一行一条，有重复。 请用 5 分钟时间，找出重复出现最多的前 10 条。

解题方案一： 常规方法是先排序，在遍历一次，找出重复最多的前 10 条。但是排序的算法复杂度最低为nlgn。   

解题方案二：可以设计一个 hash\_table, hash\_map&lt;string, int&gt;，依次读取一千万条短信，加载到hash\_table 表中，并且统计重复的次数，与此同时维护一张最多 10 条的短信表。 这样遍历一次就能找出最多的前 10 条，算法复杂度为 O\(n\)。  


### 试题九：两个文件合并的问题：给定a、b两个文件，各存放50亿个url，每个url各占用64字节，内存限制是4G，如何找出a、b文件共同的url？

解题方案一：主要的思想是把文件分开进行计算，在对每个文件进行对比，得出相同的URL,因为以上说是含有相同的URL所以不用考虑数据倾斜的问题。详细的解题思路如下： a、可以估计每个文件的大小为5G\*64=300G，远大于4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。 b、遍历文件a，对每个url求取hash\(url\)%1000，然后根据所得值将url分别存储到1000个小文件（设为a0,a1,...a999）当中。这样每个小文件的大小约为300M。 b、遍历文件b，采取和a相同的方法将url分别存储到1000个小文件\(b0,b1....b999\)中。这样处理后，所有可能相同的url都在对应的小文件\(a0 vs b0, a1 vs b1....a999 vs b999\)当中，不对应的小文件（比如a0 vs b99）不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。 c、比如对于a0 vs b0，我们可以遍历a0，将其中的url存储到hash\_map当中。然后遍历b0，如果url在hash\_map中，则说明此url在a和b中同时存在，保存到文件中即可。 d、如果分成的小文件不均匀，导致有些小文件太大（比如大于2G），可以考虑将这些太大的小文件再按类似的方法分成小小文件即可

试题十： 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10

解题方案一：

在每台电脑上求出TOP10，可以采用包含10个元素的堆完成\(TOP10小，用最大堆，TOP10大，用最小堆\)。 比如求TOP10大，我们首先取前10个元素调整成最小堆，如果发现，然后扫描后面的数据，并与堆顶元素比较，如果比堆顶元素大，那么用该元素替换堆顶，然后再调整为最小堆。 最后堆中的元素就是TOP10大。

解题方案二：

求出每台电脑上的TOP10后，然后把这100台电脑上的TOP10组合起来，共1000个数据

再利用上面类似的方法求出TOP10就可以了。

